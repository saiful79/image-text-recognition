{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NH-B4hb1E00o"
   },
   "source": [
    "## Install dependencies for keras `plot_model()` function\n",
    "restart the runtime after running this cell by\n",
    "\n",
    "`Runtime --> Restart runtime...`\n",
    "\n",
    "Then continue running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7776,
     "status": "ok",
     "timestamp": 1526105952629,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "1wsDPx682A7H",
    "outputId": "88ee055c-b674-4a81-c869-8401551f09d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import codecs\n",
    "import re\n",
    "import datetime\n",
    "import cairocffi as cairo\n",
    "import editdistance\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "# import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Reshape, Lambda\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Xnj6R_OE3yMl"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'image_ocr'\n",
    "\n",
    "# character classes and matching regex filter\n",
    "regex = r'^[a-z ]+$'\n",
    "alphabet = u'abcdefghijklmnopqrstuvwxyz '\n",
    "\n",
    "np.random.seed(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mM0eHsJO5bP6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# this creates larger \"blotches\" of noise which look\n",
    "# more realistic than just adding gaussian noise\n",
    "# assumes greyscale with pixels ranging from 0 to 1\n",
    "\n",
    "def speckle(img):\n",
    "    severity = np.random.uniform(0, 0.6)\n",
    "    blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1)\n",
    "    img_speck = (img + blur)\n",
    "    img_speck[img_speck > 1] = 1\n",
    "    img_speck[img_speck <= 0] = 0\n",
    "    return img_speck\n",
    "\n",
    "\n",
    "# paints the string in a random location the bounding box\n",
    "# also uses a random font, a slight random rotation,\n",
    "# and a random amount of speckle noise\n",
    "\n",
    "def paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False):\n",
    "    surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h)\n",
    "    with cairo.Context(surface) as context:\n",
    "        context.set_source_rgb(1, 1, 1)  # White\n",
    "        context.paint()\n",
    "        # this font list works in CentOS 7\n",
    "        if multi_fonts:\n",
    "            fonts = ['Century Schoolbook', 'Courier', 'STIX', 'URW Chancery L', 'FreeMono']\n",
    "            context.select_font_face(np.random.choice(fonts), cairo.FONT_SLANT_NORMAL,\n",
    "                                     np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL]))\n",
    "        else:\n",
    "            context.select_font_face('Courier', cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_BOLD)\n",
    "        context.set_font_size(25)\n",
    "        box = context.text_extents(text)\n",
    "        border_w_h = (4, 4)\n",
    "        if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]):\n",
    "            raise IOError('Could not fit string into image. Max char count is too large for given image width.')\n",
    "\n",
    "        # teach the RNN translational invariance by\n",
    "        # fitting text box randomly on canvas, with some room to rotate\n",
    "        max_shift_x = w - box[2] - border_w_h[0]\n",
    "        max_shift_y = h - box[3] - border_w_h[1]\n",
    "        top_left_x = np.random.randint(0, int(max_shift_x))\n",
    "        if ud:\n",
    "            top_left_y = np.random.randint(0, int(max_shift_y))\n",
    "        else:\n",
    "            top_left_y = h // 2\n",
    "        context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1]))\n",
    "        context.set_source_rgb(0, 0, 0)\n",
    "        context.show_text(text)\n",
    "\n",
    "    buf = surface.get_data()\n",
    "    a = np.frombuffer(buf, np.uint8)\n",
    "    a.shape = (h, w, 4)\n",
    "    a = a[:, :, 0]  # grab single channel\n",
    "    a = a.astype(np.float32) / 255\n",
    "    a = np.expand_dims(a, 0)\n",
    "    if rotate:\n",
    "        a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1)\n",
    "    a = speckle(a)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def shuffle_mats_or_lists(matrix_list, stop_ind=None):\n",
    "    ret = []\n",
    "    assert all([len(i) == len(matrix_list[0]) for i in matrix_list])\n",
    "    len_val = len(matrix_list[0])\n",
    "    if stop_ind is None:\n",
    "        stop_ind = len_val\n",
    "    assert stop_ind <= len_val\n",
    "\n",
    "    a = list(range(stop_ind))\n",
    "    np.random.shuffle(a)\n",
    "    a += list(range(stop_ind, len_val))\n",
    "    for mat in matrix_list:\n",
    "        if isinstance(mat, np.ndarray):\n",
    "            ret.append(mat[a])\n",
    "        elif isinstance(mat, list):\n",
    "            ret.append([mat[i] for i in a])\n",
    "        else:\n",
    "            raise TypeError('`shuffle_mats_or_lists` only supports '\n",
    "                            'numpy.array and list objects.')\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Translation of characters to unique integer values\n",
    "def text_to_labels(text):\n",
    "    ret = []\n",
    "    for char in text:\n",
    "        ret.append(alphabet.find(char))\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Reverse translation of numerical classes back to characters\n",
    "def labels_to_text(labels):\n",
    "    ret = []\n",
    "    for c in labels:\n",
    "        if c == len(alphabet):  # CTC Blank\n",
    "            ret.append(\"\")\n",
    "        else:\n",
    "            ret.append(alphabet[c])\n",
    "    return \"\".join(ret)\n",
    "\n",
    "\n",
    "# only a-z and space..probably not to difficult\n",
    "# to expand to uppercase and symbols\n",
    "\n",
    "def is_valid_str(in_str):\n",
    "    search = re.compile(regex, re.UNICODE).search\n",
    "    return bool(search(in_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HBLwtxF35f0c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Uses generator functions to supply train/test with\n",
    "# data. Image renderings are text are created on the fly\n",
    "# each time with random perturbations\n",
    "\n",
    "class TextImageGenerator(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, monogram_file, bigram_file, minibatch_size,\n",
    "                 img_w, img_h, downsample_factor, val_split,\n",
    "                 absolute_max_string_len=16):\n",
    "\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.monogram_file = monogram_file\n",
    "        self.bigram_file = bigram_file\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.val_split = val_split\n",
    "        self.blank_label = self.get_output_size() - 1\n",
    "        self.absolute_max_string_len = absolute_max_string_len\n",
    "\n",
    "    def get_output_size(self):\n",
    "        return len(alphabet) + 1\n",
    "\n",
    "    # num_words can be independent of the epoch size due to the use of generators\n",
    "    # as max_string_len grows, num_words can grow\n",
    "    def build_word_list(self, num_words, max_string_len=None, mono_fraction=0.5):\n",
    "        assert max_string_len <= self.absolute_max_string_len\n",
    "        assert num_words % self.minibatch_size == 0\n",
    "        assert (self.val_split * num_words) % self.minibatch_size == 0\n",
    "        self.num_words = num_words\n",
    "        self.string_list = [''] * self.num_words\n",
    "        tmp_string_list = []\n",
    "        self.max_string_len = max_string_len\n",
    "        self.Y_data = np.ones([self.num_words, self.absolute_max_string_len]) * -1\n",
    "        self.X_text = []\n",
    "        self.Y_len = [0] * self.num_words\n",
    "\n",
    "        # monogram file is sorted by frequency in english speech\n",
    "        with codecs.open(self.monogram_file, mode='r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if len(tmp_string_list) == int(self.num_words * mono_fraction):\n",
    "                    break\n",
    "                word = line.rstrip()\n",
    "                if max_string_len == -1 or max_string_len is None or len(word) <= max_string_len:\n",
    "                    tmp_string_list.append(word)\n",
    "\n",
    "        # bigram file contains common word pairings in english speech\n",
    "        with codecs.open(self.bigram_file, mode='r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if len(tmp_string_list) == self.num_words:\n",
    "                    break\n",
    "                columns = line.lower().split()\n",
    "                word = columns[0] + ' ' + columns[1]\n",
    "                if is_valid_str(word) and \\\n",
    "                        (max_string_len == -1 or max_string_len is None or len(word) <= max_string_len):\n",
    "                    tmp_string_list.append(word)\n",
    "        if len(tmp_string_list) != self.num_words:\n",
    "            raise IOError('Could not pull enough words from supplied monogram and bigram files. ')\n",
    "        # interlace to mix up the easy and hard words\n",
    "        self.string_list[::2] = tmp_string_list[:self.num_words // 2]\n",
    "        self.string_list[1::2] = tmp_string_list[self.num_words // 2:]\n",
    "\n",
    "        for i, word in enumerate(self.string_list):\n",
    "            self.Y_len[i] = len(word)\n",
    "            self.Y_data[i, 0:len(word)] = text_to_labels(word)\n",
    "            self.X_text.append(word)\n",
    "        self.Y_len = np.expand_dims(np.array(self.Y_len), 1)\n",
    "\n",
    "        self.cur_val_index = self.val_split\n",
    "        self.cur_train_index = 0\n",
    "\n",
    "    # each time an image is requested from train/val/test, a new random\n",
    "    # painting of the text is performed\n",
    "    def get_batch(self, index, size, train):\n",
    "        # width and height are backwards from typical Keras convention\n",
    "        # because width is the time dimension when it gets fed into the RNN\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            X_data = np.ones([size, 1, self.img_w, self.img_h])\n",
    "        else:\n",
    "            X_data = np.ones([size, self.img_w, self.img_h, 1])\n",
    "\n",
    "        labels = np.ones([size, self.absolute_max_string_len])\n",
    "        input_length = np.zeros([size, 1])\n",
    "        label_length = np.zeros([size, 1])\n",
    "        source_str = []\n",
    "        for i in range(size):\n",
    "            # Mix in some blank inputs.  This seems to be important for\n",
    "            # achieving translational invariance\n",
    "            if train and i > size - 4:\n",
    "                if K.image_data_format() == 'channels_first':\n",
    "                    X_data[i, 0, 0:self.img_w, :] = self.paint_func('')[0, :, :].T\n",
    "                else:\n",
    "                    X_data[i, 0:self.img_w, :, 0] = self.paint_func('',)[0, :, :].T\n",
    "                labels[i, 0] = self.blank_label\n",
    "                input_length[i] = self.img_w // self.downsample_factor - 2\n",
    "                label_length[i] = 1\n",
    "                source_str.append('')\n",
    "            else:\n",
    "                if K.image_data_format() == 'channels_first':\n",
    "                    X_data[i, 0, 0:self.img_w, :] = self.paint_func(self.X_text[index + i])[0, :, :].T\n",
    "                else:\n",
    "                    X_data[i, 0:self.img_w, :, 0] = self.paint_func(self.X_text[index + i])[0, :, :].T\n",
    "                labels[i, :] = self.Y_data[index + i]\n",
    "                input_length[i] = self.img_w // self.downsample_factor - 2\n",
    "                label_length[i] = self.Y_len[index + i]\n",
    "                source_str.append(self.X_text[index + i])\n",
    "        inputs = {'the_input': X_data,\n",
    "                  'the_labels': labels,\n",
    "                  'input_length': input_length,\n",
    "                  'label_length': label_length,\n",
    "                  'source_str': source_str  # used for visualization only\n",
    "                  }\n",
    "        outputs = {'ctc': np.zeros([size])}  # dummy data for dummy loss function\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    def next_train(self):\n",
    "        while 1:\n",
    "            ret = self.get_batch(self.cur_train_index, self.minibatch_size, train=True)\n",
    "            self.cur_train_index += self.minibatch_size\n",
    "            if self.cur_train_index >= self.val_split:\n",
    "                self.cur_train_index = self.cur_train_index % 32\n",
    "                (self.X_text, self.Y_data, self.Y_len) = shuffle_mats_or_lists(\n",
    "                    [self.X_text, self.Y_data, self.Y_len], self.val_split)\n",
    "            yield ret\n",
    "\n",
    "    def next_val(self):\n",
    "        while 1:\n",
    "            ret = self.get_batch(self.cur_val_index, self.minibatch_size, train=False)\n",
    "            self.cur_val_index += self.minibatch_size\n",
    "            if self.cur_val_index >= self.num_words:\n",
    "                self.cur_val_index = self.val_split + self.cur_val_index % 32\n",
    "            yield ret\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.build_word_list(16000, 4, 1)\n",
    "        self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n",
    "                                                  rotate=False, ud=False, multi_fonts=False)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        # rebind the paint function to implement curriculum learning\n",
    "        if 3 <= epoch < 6:\n",
    "            self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n",
    "                                                      rotate=False, ud=True, multi_fonts=False)\n",
    "        elif 6 <= epoch < 9:\n",
    "            self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n",
    "                                                      rotate=False, ud=True, multi_fonts=True)\n",
    "        elif epoch >= 9:\n",
    "            self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n",
    "                                                      rotate=True, ud=True, multi_fonts=True)\n",
    "        if epoch >= 21 and self.max_string_len < 12:\n",
    "            self.build_word_list(32000, 12, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YOAsKfXm5jP1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# the actual loss calc occurs here despite it not being\n",
    "# an internal Keras loss function\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "\n",
    "# For a real OCR application, this should be beam search with a dictionary\n",
    "# and language model.  For this example, best path is sufficient.\n",
    "\n",
    "def decode_batch(test_func, word_batch):\n",
    "    out = test_func([word_batch])[0]\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        outstr = labels_to_text(out_best)\n",
    "        ret.append(outstr)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KcGtIsLF5leW"
   },
   "outputs": [],
   "source": [
    "class VizCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, run_name, test_func, text_img_gen, num_display_words=6):\n",
    "        self.test_func = test_func\n",
    "        self.output_dir = os.path.join(\n",
    "            OUTPUT_DIR, run_name)\n",
    "        self.text_img_gen = text_img_gen\n",
    "        self.num_display_words = num_display_words\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def show_edit_distance(self, num):\n",
    "        num_left = num\n",
    "        mean_norm_ed = 0.0\n",
    "        mean_ed = 0.0\n",
    "        while num_left > 0:\n",
    "            word_batch = next(self.text_img_gen)[0]\n",
    "            num_proc = min(word_batch['the_input'].shape[0], num_left)\n",
    "            decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc])\n",
    "            for j in range(num_proc):\n",
    "                edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j])\n",
    "                mean_ed += float(edit_dist)\n",
    "                mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j])\n",
    "            num_left -= num_proc\n",
    "        mean_norm_ed = mean_norm_ed / num\n",
    "        mean_ed = mean_ed / num\n",
    "        print('\\nOut of %d samples:  Mean edit distance: %.3f Mean normalized edit distance: %0.3f'\n",
    "              % (num, mean_ed, mean_norm_ed))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.model.save_weights(os.path.join(self.output_dir, 'weights%02d.h5' % (epoch)))\n",
    "        self.show_edit_distance(256)\n",
    "        word_batch = next(self.text_img_gen)[0]\n",
    "        res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words])\n",
    "        if word_batch['the_input'][0].shape[0] < 256:\n",
    "            cols = 2\n",
    "        else:\n",
    "            cols = 1\n",
    "        for i in range(self.num_display_words):\n",
    "            plt.subplot(self.num_display_words // cols, cols, i + 1)\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                the_input = word_batch['the_input'][i, 0, :, :]\n",
    "            else:\n",
    "                the_input = word_batch['the_input'][i, :, :, 0]\n",
    "            plt.imshow(the_input.T, cmap='Greys_r')\n",
    "            plt.xlabel('Truth = \\'%s\\'\\nDecoded = \\'%s\\'' % (word_batch['source_str'][i], res[i]))\n",
    "        fig = pylab.gcf()\n",
    "        fig.set_size_inches(10, 13)\n",
    "        plt.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch)))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eC4vEfcQ5oOq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(run_name, start_epoch, stop_epoch, img_w):\n",
    "    # Input Parameters\n",
    "    img_h = 64\n",
    "    words_per_epoch = 16000\n",
    "    val_split = 0.2\n",
    "    val_words = int(words_per_epoch * (val_split))\n",
    "\n",
    "    # Network parameters\n",
    "    conv_filters = 16\n",
    "    kernel_size = (3, 3)\n",
    "    pool_size = 2\n",
    "    time_dense_size = 32\n",
    "    rnn_size = 512\n",
    "    minibatch_size = 32\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, img_w, img_h)\n",
    "    else:\n",
    "        input_shape = (img_w, img_h, 1)\n",
    "\n",
    "    fdir = os.path.dirname(get_file('wordlists.tgz',\n",
    "                                    origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True))\n",
    "\n",
    "    img_gen = TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n",
    "                                 bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n",
    "                                 minibatch_size=minibatch_size,\n",
    "                                 img_w=img_w,\n",
    "                                 img_h=img_h,\n",
    "                                 downsample_factor=(pool_size ** 2),\n",
    "                                 val_split=words_per_epoch - val_words\n",
    "                                 )\n",
    "    act = 'relu'\n",
    "    input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
    "    inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
    "                   activation=act, kernel_initializer='he_normal',\n",
    "                   name='conv1')(input_data)\n",
    "    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n",
    "    inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
    "                   activation=act, kernel_initializer='he_normal',\n",
    "                   name='conv2')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n",
    "\n",
    "    conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\n",
    "    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n",
    "\n",
    "    # cuts down input size going into RNN:\n",
    "    inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n",
    "\n",
    "    # Two layers of bidirectional GRUs\n",
    "    # GRU seems to work as well, if not better than LSTM:\n",
    "    gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner)\n",
    "    gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner)\n",
    "    gru1_merged = add([gru_1, gru_1b])\n",
    "    gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged)\n",
    "    gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    inner = Dense(img_gen.get_output_size(), kernel_initializer='he_normal',\n",
    "                  name='dense2')(concatenate([gru_2, gru_2b]))\n",
    "    y_pred = Activation('softmax', name='softmax')(inner)\n",
    "    Model(inputs=input_data, outputs=y_pred).summary()\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[img_gen.absolute_max_string_len], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    # Keras doesn't currently support loss funcs with extra parameters\n",
    "    # so CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "    # clipnorm seems to speeds up convergence\n",
    "    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "    model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "    if start_epoch > 0:\n",
    "        weight_file = os.path.join(OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1)))\n",
    "        model.load_weights(weight_file)\n",
    "    # captures output of softmax so we can decode the output during visualization\n",
    "    test_func = K.function([input_data], [y_pred])\n",
    "\n",
    "    viz_cb = VizCallback(run_name, test_func, img_gen.next_val())\n",
    "\n",
    "    model.fit_generator(generator=img_gen.next_train(),\n",
    "                        steps_per_epoch=(words_per_epoch - val_words) // minibatch_size,\n",
    "                        epochs=stop_epoch,\n",
    "                        validation_data=img_gen.next_val(),\n",
    "                        validation_steps=val_words // minibatch_size,\n",
    "                        callbacks=[viz_cb, img_gen],\n",
    "                        initial_epoch=start_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13kO-MxI7XWb"
   },
   "source": [
    "### Start the training\n",
    "It may take an hour, alternative you can just download my pre-trained model weights.\n",
    "```shell\n",
    "! wget https://github.com/Tony607/keras-image-ocr/releases/download/V0.1/weights24.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2094
    },
    "colab_type": "code",
    "id": "YRGglF5L5qzn",
    "outputId": "541e795b-80d1-49e6-9c9f-e2014690f4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.mythic-ai.com/datasets/wordlists.tgz\n",
      "770048/768223 [==============================] - 0s 0us/step\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 128, 64, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 64, 16)  160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 64, 32, 16)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 64, 32, 16)   2320        max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 32, 16, 16)   0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 256)      0           max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32, 32)       8224        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru1 (GRU)                      (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru1_b (GRU)                    (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 512)      0           gru1[0][0]                       \n",
      "                                                                 gru1_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru2 (GRU)                      (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru2_b (GRU)                    (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 1024)     0           gru2[0][0]                       \n",
      "                                                                 gru2_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32, 28)       28700       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 32, 28)       0           dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,862,444\n",
      "Trainable params: 4,862,444\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/20\n",
      " 21/400 [>.............................] - ETA: 2:59 - loss: 24.5378400/400 [==============================] - 133s 333ms/step - loss: 12.3487 - val_loss: 10.4443\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 2.754 Mean normalized edit distance: 0.781\n",
      "Epoch 2/20\n",
      "287/400 [====================>.........] - ETA: 33s - loss: 6.3274400/400 [==============================] - 129s 323ms/step - loss: 5.4518 - val_loss: 3.5311\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 1.062 Mean normalized edit distance: 0.295\n",
      "Epoch 3/20\n",
      "360/400 [==========================>...] - ETA: 11s - loss: 0.8411400/400 [==============================] - 130s 324ms/step - loss: 0.7553 - val_loss: 0.0340\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.016 Mean normalized edit distance: 0.006\n",
      "Epoch 4/20\n",
      "378/400 [===========================>..] - ETA: 6s - loss: 5.3351400/400 [==============================] - 129s 323ms/step - loss: 5.0730 - val_loss: 0.9439\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.305 Mean normalized edit distance: 0.088\n",
      "Epoch 5/20\n",
      "384/400 [===========================>..] - ETA: 4s - loss: 0.3049400/400 [==============================] - 129s 323ms/step - loss: 0.2983 - val_loss: 0.2261\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.051 Mean normalized edit distance: 0.015\n",
      "Epoch 6/20\n",
      "385/400 [===========================>..] - ETA: 4s - loss: 0.1098400/400 [==============================] - 129s 322ms/step - loss: 0.1104 - val_loss: 0.2191\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.051 Mean normalized edit distance: 0.015\n",
      "Epoch 7/20\n",
      "385/400 [===========================>..] - ETA: 4s - loss: 0.8603400/400 [==============================] - 129s 322ms/step - loss: 0.8391 - val_loss: 0.4505\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.125 Mean normalized edit distance: 0.033\n",
      "Epoch 8/20\n",
      "385/400 [===========================>..] - ETA: 4s - loss: 0.2658400/400 [==============================] - 129s 322ms/step - loss: 0.2642 - val_loss: 0.2280\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.078 Mean normalized edit distance: 0.022\n",
      "Epoch 9/20\n",
      "385/400 [===========================>..] - ETA: 4s - loss: 0.1978400/400 [==============================] - 131s 326ms/step - loss: 0.1949 - val_loss: 0.2185\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.059 Mean normalized edit distance: 0.017\n",
      "Epoch 10/20\n",
      "384/400 [===========================>..] - ETA: 5s - loss: 0.2668400/400 [==============================] - 138s 344ms/step - loss: 0.2624 - val_loss: 0.3137\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.117 Mean normalized edit distance: 0.033\n",
      "Epoch 11/20\n",
      "384/400 [===========================>..] - ETA: 4s - loss: 0.1606400/400 [==============================] - 135s 337ms/step - loss: 0.1591 - val_loss: 0.1471\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.043 Mean normalized edit distance: 0.013\n",
      "Epoch 12/20\n",
      "384/400 [===========================>..] - ETA: 4s - loss: 0.1270400/400 [==============================] - 135s 338ms/step - loss: 0.1245 - val_loss: 0.4319\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.133 Mean normalized edit distance: 0.037\n",
      "Epoch 13/20\n",
      "384/400 [===========================>..] - ETA: 4s - loss: 0.1239400/400 [==============================] - 136s 340ms/step - loss: 0.1217 - val_loss: 0.1755\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.062 Mean normalized edit distance: 0.018\n",
      "Epoch 14/20\n",
      "384/400 [===========================>..] - ETA: 4s - loss: 0.0550400/400 [==============================] - 136s 339ms/step - loss: 0.0558 - val_loss: 0.2097\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.074 Mean normalized edit distance: 0.021\n",
      "Epoch 15/20\n",
      "384/400 [===========================>..] - ETA: 4s - loss: 0.0608400/400 [==============================] - 136s 341ms/step - loss: 0.0608 - val_loss: 0.1681\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.043 Mean normalized edit distance: 0.012\n",
      "Epoch 16/20\n",
      "383/400 [===========================>..] - ETA: 5s - loss: 0.0589400/400 [==============================] - 136s 341ms/step - loss: 0.0588 - val_loss: 0.1111\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.020 Mean normalized edit distance: 0.005\n",
      "Epoch 17/20\n",
      "383/400 [===========================>..] - ETA: 5s - loss: 0.0369400/400 [==============================] - 135s 338ms/step - loss: 0.0378 - val_loss: 0.1912\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.043 Mean normalized edit distance: 0.013\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/400 [===========================>..] - ETA: 5s - loss: 0.0426400/400 [==============================] - 135s 339ms/step - loss: 0.0472 - val_loss: 0.1881\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.074 Mean normalized edit distance: 0.021\n",
      "Epoch 19/20\n",
      "383/400 [===========================>..] - ETA: 5s - loss: 0.0199400/400 [==============================] - 138s 346ms/step - loss: 0.0189 - val_loss: 0.0734\n",
      "\n",
      "Out of 256 samples:  Mean edit distance: 0.047 Mean normalized edit distance: 0.014\n",
      "Epoch 20/20\n",
      "138/400 [=========>....................] - ETA: 1:23 - loss: -5.7473e-04"
     ]
    }
   ],
   "source": [
    "run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S')\n",
    "train(run_name, 0, 20, 128)\n",
    "# increase to wider images and start at epoch 20. The learned weights are reloaded\n",
    "train(run_name, 20, 25, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4jVRC6J3vUy"
   },
   "source": [
    "## Predict\n",
    "Download a pre-trained weights file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2570,
     "status": "ok",
     "timestamp": 1526105960543,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "1TIu0VHI3ze-",
    "outputId": "db31098f-cc67-4fb4-e0f2-176e9c01b49a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-26 23:38:03--  https://github.com/Tony607/keras-image-ocr/releases/download/V0.1/weights24.h5\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/133029758/81b50290-5551-11e8-98c4-db72d80e52fc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191126%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191126T173804Z&X-Amz-Expires=300&X-Amz-Signature=9a544153e0bfe902257e421cb8b211479965758d41fa95d2633a125f912e5016&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dweights24.h5&response-content-type=application%2Foctet-stream [following]\n",
      "--2019-11-26 23:38:04--  https://github-production-release-asset-2e65be.s3.amazonaws.com/133029758/81b50290-5551-11e8-98c4-db72d80e52fc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191126%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191126T173804Z&X-Amz-Expires=300&X-Amz-Signature=9a544153e0bfe902257e421cb8b211479965758d41fa95d2633a125f912e5016&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dweights24.h5&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 54.231.40.3\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|54.231.40.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19490408 (19M) [application/octet-stream]\n",
      "Saving to: ‘weights24.h5’\n",
      "\n",
      "weights24.h5        100%[===================>]  18.59M   875KB/s    in 57s     \n",
      "\n",
      "2019-11-26 23:39:02 (336 KB/s) - ‘weights24.h5’ saved [19490408/19490408]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Tony607/keras-image-ocr/releases/download/V0.1/weights24.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5918,
     "status": "ok",
     "timestamp": 1526105966476,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "on6qCFcV5uPx",
    "outputId": "3dcc507c-3baa-4d00-d38f-eb2e65d840b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saiful/anaconda3/envs/tf3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 128, 64, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 64, 16)  160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 64, 32, 16)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 64, 32, 16)   2320        max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 32, 16, 16)   0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 256)      0           max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32, 32)       8224        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru1 (GRU)                      (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru1_b (GRU)                    (None, 32, 512)      837120      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 512)      0           gru1[0][0]                       \n",
      "                                                                 gru1_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru2 (GRU)                      (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru2_b (GRU)                    (None, 32, 512)      1574400     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 1024)     0           gru2[0][0]                       \n",
      "                                                                 gru2_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32, 28)       28700       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 32, 28)       0           dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,862,444\n",
      "Trainable params: 4,862,444\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/saiful/anaconda3/envs/tf3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4249: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/saiful/anaconda3/envs/tf3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4229: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "weight_file = './weights24.h5'\n",
    "img_w = 128\n",
    "# Input Parameters\n",
    "img_h = 64\n",
    "words_per_epoch = 16000\n",
    "val_split = 0.2\n",
    "val_words = int(words_per_epoch * (val_split))\n",
    "\n",
    "# Network parameters\n",
    "conv_filters = 16\n",
    "kernel_size = (3, 3)\n",
    "pool_size = 2\n",
    "time_dense_size = 32\n",
    "rnn_size = 512\n",
    "minibatch_size = 32\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, img_w, img_h)\n",
    "else:\n",
    "    input_shape = (img_w, img_h, 1)\n",
    "\n",
    "fdir = os.path.dirname(get_file('wordlists.tgz',\n",
    "                                origin='http://www.mythic-ai.com/datasets/wordlists.tgz', untar=True))\n",
    "\n",
    "img_gen = TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n",
    "                             bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n",
    "                             minibatch_size=minibatch_size,\n",
    "                             img_w=img_w,\n",
    "                             img_h=img_h,\n",
    "                             downsample_factor=(pool_size ** 2),\n",
    "                             val_split=words_per_epoch - val_words\n",
    "                             )\n",
    "act = 'relu'\n",
    "input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
    "inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
    "               activation=act, kernel_initializer='he_normal',\n",
    "               name='conv1')(input_data)\n",
    "inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n",
    "inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
    "               activation=act, kernel_initializer='he_normal',\n",
    "               name='conv2')(inner)\n",
    "inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n",
    "\n",
    "conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\n",
    "inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n",
    "\n",
    "# cuts down input size going into RNN:\n",
    "inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n",
    "\n",
    "# Two layers of bidirectional GRUs\n",
    "# GRU seems to work as well, if not better than LSTM:\n",
    "gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner)\n",
    "gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner)\n",
    "gru1_merged = add([gru_1, gru_1b])\n",
    "gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged)\n",
    "gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n",
    "\n",
    "# transforms RNN output to character activations:\n",
    "inner = Dense(img_gen.get_output_size(), kernel_initializer='he_normal',\n",
    "              name='dense2')(concatenate([gru_2, gru_2b]))\n",
    "y_pred = Activation('softmax', name='softmax')(inner)\n",
    "Model(inputs=input_data, outputs=y_pred).summary()\n",
    "\n",
    "labels = Input(name='the_labels', shape=[img_gen.absolute_max_string_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "# Keras doesn't currently support loss funcs with extra parameters\n",
    "# so CTC loss is implemented in a lambda layer\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "# clipnorm seems to speeds up convergence\n",
    "sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "model.load_weights(weight_file)\n",
    "# captures output of softmax so we can decode the output during visualization\n",
    "test_func = K.function([input_data], [y_pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1549
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2778,
     "status": "ok",
     "timestamp": 1526105969268,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "_f_3wfZr36dq",
    "outputId": "c360da6c-190e-4a05-fa1c-46f8018161b9"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1914\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m                 \u001b[0mworking_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcall_graphviz\u001b[0;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    706\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1332\u001b[0m                                 \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_executable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     prog=prog)\n\u001b[0;32m-> 1922\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1923\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e76f723927a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         raise OSError(\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[0;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "from IPython.display import Image\n",
    "Image(filename='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YrSqXKIY4WRU"
   },
   "outputs": [],
   "source": [
    "model_p = Model(inputs=input_data, outputs=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SDkiHH8_4aVJ"
   },
   "outputs": [],
   "source": [
    "def decode_predict_ctc(out, top_paths = 1):\n",
    "    results = []\n",
    "    beam_width = 5\n",
    "    if beam_width < top_paths:\n",
    "      beam_width = top_paths\n",
    "    for i in range(top_paths):\n",
    "      lables = K.get_value(K.ctc_decode(out, input_length=np.ones(out.shape[0])*out.shape[1],\n",
    "                           greedy=False, beam_width=beam_width, top_paths=top_paths)[0][i])[0]\n",
    "      text = labels_to_text(lables)\n",
    "      results.append(text)\n",
    "    return results\n",
    "  \n",
    "def predit_a_image(a, top_paths = 1):\n",
    "  c = np.expand_dims(a.T, axis=0)\n",
    "  net_out_value = model_p.predict(c)\n",
    "  top_pred_texts = decode_predict_ctc(net_out_value, top_paths)\n",
    "  return top_pred_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1526107425874,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "gVH2yF7DAm10",
    "outputId": "8e483e0f-87a3-4302-bb83-4e28497d135b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saiful/anaconda3/envs/tf3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4303: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a ae']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 64\n",
    "w = 128\n",
    "a = paint_text('a game',h = h, w = w)\n",
    "b = a.reshape((h, w))\n",
    "plt.imshow(b, cmap='Greys_r')\n",
    "plt.show()\n",
    "\n",
    "c = np.expand_dims(a.T, axis=0)\n",
    "\n",
    "net_out_value = model_p.predict(c)\n",
    "pred_texts = decode_predict_ctc(net_out_value)\n",
    "pred_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1526107438636,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "Zzahy6G-HXy4",
    "outputId": "2a92f768-1cd5-4456-8994-073886b1205f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a ae', 'a ame', 'a aoe']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predit_a_image(a, top_paths = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1526105974272,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "RPuabD6PAqFR",
    "outputId": "a45dfe84-156d-4290-9cfd-601a24b27684"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAFKCAYAAAAJ5nSzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYQcAYwg7ABhD2AHAGMIOAMb8H4ZgmHQE93DlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a49f00e48>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(net_out_value[0].T, cmap='binary', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1526105974956,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "9M94ioRQA217",
    "outputId": "3b1d8af0-4768-4954-bba3-fdb81df9e9c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 26,  6,  0, 12,  4]])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.get_value(K.ctc_decode(net_out_value, input_length=np.ones(net_out_value.shape[0])*net_out_value.shape[1],\n",
    "                         greedy=False, beam_width=3, top_paths=3)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1526106981099,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "VeB1FAyVIpl_",
    "outputId": "bec04e29-83e9-4614-8c31-700a16b7b898"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor 'SparseToDense_25:0' shape=(?, ?) dtype=int64>,\n",
       "  <tf.Tensor 'SparseToDense_26:0' shape=(?, ?) dtype=int64>,\n",
       "  <tf.Tensor 'SparseToDense_27:0' shape=(?, ?) dtype=int64>,\n",
       "  <tf.Tensor 'SparseToDense_28:0' shape=(?, ?) dtype=int64>,\n",
       "  <tf.Tensor 'SparseToDense_29:0' shape=(?, ?) dtype=int64>,\n",
       "  <tf.Tensor 'SparseToDense_30:0' shape=(?, ?) dtype=int64>],\n",
       " <tf.Tensor 'CTCBeamSearchDecoder_8:18' shape=(1, 6) dtype=float32>)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.ctc_decode(net_out_value, input_length=np.ones(net_out_value.shape[0])*net_out_value.shape[1],\n",
    "                         greedy=False, beam_width=5, top_paths=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1681,
     "status": "ok",
     "timestamp": 1526105901853,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "b0NDGYZwKElX",
    "outputId": "e8395f8a-b8c9-46dc-b174-dba3de6fe884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1392,
     "status": "ok",
     "timestamp": 1526106995201,
     "user": {
      "displayName": "Chengwei Zhang",
      "photoUrl": "//lh5.googleusercontent.com/-FK2ckwmh6mM/AAAAAAAAAAI/AAAAAAAAPLw/SX9b1QAzJ5g/s50-c-k-no/photo.jpg",
      "userId": "114808171854651597062"
     },
     "user_tz": -480
    },
    "id": "Qe-RceTPK-yJ",
    "outputId": "7081439c-61f3-4e2d-f293-5c68721d6496"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 26,  6,  0, 12,  4]])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.get_value(K.ctc_decode(net_out_value, input_length=np.ones(net_out_value.shape[0])*net_out_value.shape[1],\n",
    "                         greedy=False, beam_width=3, top_paths=3)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qz7VeMhxC7ID"
   },
   "outputs": [],
   "source": [
    "K.ctc_decode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aCAJ0joMDJWO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "image_ocr.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
